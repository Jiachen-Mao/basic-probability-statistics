{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "\n",
    "# Drop columns with missing value rate higher than threshold\n",
    "data = data[data.columns[data.isnull().mean < threshold]]\n",
    "\n",
    "# Drop rows with missing value rate higher than threshold\n",
    "data = data.loc[data.isnull().mean(axis = 1) < threshold]\n",
    "\n",
    "# Fill all missing values with 0\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Fill missing values with medians of the columns\n",
    "data = data.fillna(data.median())\n",
    "\n",
    "# Max fill function for categorical columns\n",
    "data['column_name'].fillna(data['column_name'].value_counts().idxmax(), inplace = True)\n",
    "\n",
    "data['column_name'].fillna('Other', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the outlier rows with standard deviation\n",
    "\n",
    "factor = 3\n",
    "\n",
    "upper_lim = data['column'].mean() + data['column'].std() * factor\n",
    "lower_lim = data['column'].mean() - data['column'].std() * factor\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]\n",
    "\n",
    "\n",
    "# Drop the outlier rows with percentiles\n",
    "\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & data['column'] > lower_lim]\n",
    "\n",
    "\n",
    "# Cap the outlier rows with percentiles\n",
    "\n",
    "data.loc[(df[column] > upper_lim), column] = upper_lim\n",
    "data.loc[(df[column] < lower_lim), column] = lower_lim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Transform\n",
    "\n",
    "1) It helps to handle skewed data and after transformation, the distribution becomes more normal.  \n",
    "2) It also decreases the effect of outliers, due to the normalization of magnitude difference and the model becomes more robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['log'] = (data['value'] - data['value'].min() + 1).transform(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indicator variables\n",
    "\n",
    "* Indicator variable from thresholds\n",
    "* Indicator variable from multiple features\n",
    "* Indicator variable for special events\n",
    "* Indicator variable for groups of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction features\n",
    "\n",
    "* Sum of two features\n",
    "* Difference between two features\n",
    "* Product of two features\n",
    "* Quotient of two features\n",
    "\n",
    "We do not recommend using an automated loop to create interactions for all your features. This leads to feature explosion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature representation\n",
    "\n",
    "* Date and time features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datatime import date\n",
    "\n",
    "data = pd.DataFrame({'date':\n",
    "                    ['01-01-2017',\n",
    "                    '04-12-2008',\n",
    "                    '23-06-1988',\n",
    "                    '25-08-1999',\n",
    "                    '20-02-1993']})\n",
    "\n",
    "# Transform string to date\n",
    "\n",
    "data['date'] = pd.to_datetime(data.date, format = \"%d-%m-%Y\")\n",
    "\n",
    "# Extract year\n",
    "\n",
    "data['year'] = data['date'].dt.year\n",
    "\n",
    "# Extract month\n",
    "\n",
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "# Extract passed years since the date\n",
    "\n",
    "data['passed_years'] = date.today().year - date['date'].dt.year\n",
    "\n",
    "# Extract passed months since the date\n",
    "\n",
    "data['passed_months'] = (date.today().year - data['date'].dt.year)*12 + date.today().month - data['date'].dt.month\n",
    "\n",
    "# Extract the weekday name of the date\n",
    "\n",
    "data['day_name'] = data['date'].dt.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Binning\n",
    "\n",
    "The main motivation is to make the model more robust and prevent overfitting, however, it has a cost to the performance. Every time you bin something, you sacrifice information and make your data more regularized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical binning\n",
    "\n",
    "data['bin'] = pd.cut(data['value'], bins = [0, 30, 70, 100], labels = ['low', 'mid', 'high'])\n",
    "\n",
    "# Categorical binning\n",
    "\n",
    "conditions = [\n",
    "    data['country'].str.contains('Spain'),\n",
    "    data['country'].str.contains('Italy'),\n",
    "    data['country'].str.contains('Chile'),\n",
    "    data['country'].str.contains('Brazil')\n",
    "]\n",
    "\n",
    "choices = ['Europe', 'Europe', 'South America', 'South America']\n",
    "\n",
    "data['continent'] = np.select(conditions, choices, default = 'Other')  # Group sparse classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_columns = pd.get_dummies(data['column'])\n",
    "\n",
    "data = data.join(encoded_columns).drop('column', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Labeled encoding\n",
    "* Frequency encoding\n",
    "* Target mean encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual Data\n",
    "\n",
    "* Bag-of-Words: Extract tokens from text and use their occurances as features\n",
    "* NLP techniques:\n",
    "  * Remove stop words\n",
    "  * Convert all words to lower case\n",
    "  * Stemming for English words\n",
    "  * Pingyin\n",
    "* Deep Learning for textual data\n",
    "  * Turn each token into a vector of predefined size\n",
    "  * Help compute \"semantic distance\" between tokens/words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Data\n",
    "\n",
    "* External APIs\n",
    "* Geocoding\n",
    "* Other sources of the same data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "* Start with larger errors\n",
    "* Segment by classes\n",
    "* Unsupervised clustering\n",
    "* Ask colleagues or domain experts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
